{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portugese Meals Classification: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of keras abstractions it's really hard to prune pretrained models. I'll stop at just quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open('config.yaml', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncompress_splits(dir: str):\n",
    "    X = np.load(dir + 'Xvalues.npz')['arr_0'][:100]\n",
    "    Y = np.load(dir + 'Yvalues.npz')['arr_0'][:100]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "images, labels = uncompress_splits(config['dataset']['augmented_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "  for idx in range(100):\n",
    "    data = images[idx]\n",
    "    yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.load_model(config['baseline_path'])\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32  \n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_file = pathlib.Path(config['tflite_path'])\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "\n",
    "# I'll be using INT8 since I don't have a GPU\n",
    "conversion_params = trt.TrtConversionParams(\n",
    "    precision_mode=trt.TrtPrecisionMode.INT8)\n",
    "\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "    input_saved_model_dir=config['saved_model_baseline_path'],\n",
    "    conversion_params=conversion_params)\n",
    "\n",
    "converter.convert()\n",
    "\n",
    "# converter.save(config['tensorrt_path'])\n",
    "converter.save('./model/tensorrt/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnxruntime -q\n",
    "%pip install tf2onnx\n",
    "%python -m tf2onnx.convert --saved-model tensorflow-model-path --output model.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20f5b49acdb8aa5482bf611cd625371fff780f4a1ea28b5df1442e16c229ca99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
