{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portugese Meals Classification: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import clear_output\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\GigaFolder\\projects\\Portuguese-Meals-Classification\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open('config.yaml', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncompress_splits(dir: str):\n",
    "    X = np.load(dir + 'Xvalues.npz')['arr_0']\n",
    "    Y = np.load(dir + 'Yvalues.npz')['arr_0']\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = uncompress_splits(config['dataset']['augmented_dir'])\n",
    "assert len(images) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    images, labels, train_size=config['dataset']['train_ratio'], random_state=config['random_seed'], shuffle=True, stratify=labels)\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    images, labels, train_size=config['dataset']['validation_ratio'], random_state=config['random_seed'], shuffle=True, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2M(\n",
    "    include_top=False, weights=\"imagenet\", input_shape=config['img_shape'], pooling='max')\n",
    "\n",
    "base_model.trainable = config['train']['base_trainable']\n",
    "\n",
    "x = base_model.output\n",
    "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
    "x = Dense(256, kernel_regularizer=regularizers.l2(l=0.016), activity_regularizer=regularizers.l1(0.006),\n",
    "          bias_regularizer=regularizers.l1(0.006), activation='relu')(x)\n",
    "x = Dropout(rate=.2, seed=config['random_seed'])(x)\n",
    "output = Dense(config['class_count'], activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(Adamax(learning_rate=config['train']['lr']),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=config['train']['label_smoothing']), metrics=config['train']['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to reverse one-hot encoded labels\n",
    "# I need to reverse one-hot encoded labels\n",
    "classes = ['aletria',\n",
    "           'arroz_cabidela',\n",
    "           'bacalhau_bras',\n",
    "           'bacalhau_natas',\n",
    "           'batatas_fritas',\n",
    "           'bolo_chocolate',\n",
    "           'cachorro',\n",
    "           'caldo_verde',\n",
    "           'cozido_portuguesa',\n",
    "           'croissant',\n",
    "           'donuts',\n",
    "           'esparguete_bolonhesa',\n",
    "           'feijoada',\n",
    "           'francesinha',\n",
    "           'gelado',\n",
    "           'hamburguer',\n",
    "           'jardineira',\n",
    "           'nata',\n",
    "           'ovo',\n",
    "           'pasteis_bacalhau',\n",
    "           'pizza',\n",
    "           'tripas_moda_porto',\n",
    "           'waffles']\n",
    "\n",
    "classes.sort()\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "classes_oh = np.asarray(classes)\n",
    "classes_oh = encoder.fit_transform(classes_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, image_size=config['img_shape'][0]):\n",
    "    # I'll go for a square image\n",
    "    return cv2.resize(image, (image_size, image_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def normalize(image):\n",
    "    image = image / 255\n",
    "    return image\n",
    "\n",
    "def preprocess(image):\n",
    "    image = resize(image)\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on_dataframe(model, x, y, name):\n",
    "\n",
    "    clss = encoder.inverse_transform(np.array(y))\n",
    "    preds = model.predict(x)\n",
    "\n",
    "    res = []\n",
    "    for idx in range(len(preds)):\n",
    "        res.append(np.argmax(y[idx]) == np.argmax(preds[idx]))\n",
    "        # print(f'True idx = {np.argmax(true[idx])}')\n",
    "        # print(f'Predicted idx = {np.argmax(preds[idx])}')\n",
    "    print(f'Accuracy: {np.sum(res) / np.sum(y)*100:.04}%')\n",
    "\n",
    "    preds = encoder.inverse_transform(np.array(preds))\n",
    "    cm = confusion_matrix(np.array(classes), preds)\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    cm = sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.xticks(np.arange(config['class_count'])+.5, clss, rotation=90)\n",
    "    plt.yticks(np.arange(config['class_count'])+.5, clss, rotation=0)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    # create classification report\n",
    "    clr = classification_report(clss, preds, target_names=classes, digits=4)\n",
    "    print(f\"Classification Report for {name}:\\n----------------------\\n\", clr)\n",
    "\n",
    "    wandb.log({f'confusion_matrix_{name}': wandb.Image(cm)})\n",
    "    wandb.log({f'{name}_accuracy:': np.sum(res) / np.sum(y)*100})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=config['wandb']['project'],\n",
    "                name=config['wandb']['name'],\n",
    "                job_type='Training+Validation',\n",
    "                config=config):\n",
    "    callbacks = [EarlyStopping(**config['callbacks']['EarlyStopping']),\n",
    "                 ReduceLROnPlateau(**config['callbacks']['ReduceLROnPlateau']),\n",
    "                 WandbCallback(**config['callbacks']['WandbCallback'])]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=config['train']['epochs'],\n",
    "        batch_size=config['train']['batch_size'],\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    validate_on_dataframe(model, x_val, y_val, 'baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(config['baseline_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, config['saved_model_baseline_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20f5b49acdb8aa5482bf611cd625371fff780f4a1ea28b5df1442e16c229ca99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
